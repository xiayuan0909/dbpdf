["Search-in-the-Chain: Interactively Enhancing Large Language\nModels with Search for Knowledge-intensive Tasks\nShicheng Xu\nCAS Key Laboratory of AI Security,\nInstitute of Computing Technology,\nChinese Academy of Sciences\nUniversity of Chinese Academy of\nSciences\nBeijing, China\nxushicheng21s@ict.ac.cn\nLiang Pangâˆ—\nCAS Key Laboratory of AI Security,\nInstitute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\npangliang@ict.ac.cn\nHuawei Shen\nCAS Key Laboratory of AI Security,\nInstitute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\nshenhuawei@ict.ac.cn\nXueqi Cheng\nCAS Key Laboratory of AI Security,\nInstitute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\ncxq@ict.ac.cn\nTat-Seng Chua\nSea-NExT Joint Lab, National\nUniversity of Singapore\nSingapore\ndcscts@nus.edu.sg\nABSTRACT\nMaking the content generated by Large Language Model (LLM),\naccurate, credible and traceable is crucial, especially in complex\nknowledge-intensive tasks that require multi-step reasoning and\neach step needs knowledge to solve. Retrieval-augmented genera-\ntion is good potential to solve this problem. However, where and\nhow to introduce Information Retrieval (IR) to LLM is a big chal-\nlenge. Previous work has the problems that wrong knowledge re-\ntrieved by IR misleads the LLM and interaction between IR and\nLLM breaks the reasoning chain of LLM. This paper proposes a\nnovel framework named Search-in-the-Chain (SearChain) for the\ninteraction between LLM and IR to solve the challenges. First, LLM\ngenerates the reasoning chain named Chain-of-Query (CoQ) where\neach node consists of an IR-oriented query-answer pair. Second,\nIR verifies the answer of each node of CoQ. It corrects the answer\nthat is not consistent with the retrieved information when IR gives\nhigh confidence, which improves the credibility. Third, LLM can\nindicate its missing knowledge in CoQ and rely on IR to provide\nthis knowledge to LLM. These operations improve the accuracy in\nterms of reasoning and knowledge. Finally, SearChain generates the\nreasoning process and marks references to supporting documents\nfor each reasoning step, which improves traceability. Interaction\nwith IR in SearChain forms a novel reasoning path based on a\ntree, which enables LLM to dynamically modify the direction of\nreasoning. Experiments show that SearChain outperforms state-of-\nthe-art baselines on complex knowledge-intensive tasks including\nmulti-hop Q&A, slot filling, fact checking, and long-form Q&A.\nâˆ—Corresponding author\nThis work is licensed under a Creative Commons Attribution\nInternational 4.0 License.\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nÂ© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0171-9/24/05.\nhttps://doi.org/10.1145/3589334.3645363\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Natural language processing.\nKEYWORDS\nRetrieval-augmented model, Large Language Models\nACM Reference Format:\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua.\n2024. Search-in-the-Chain: Interactively Enhancing Large Language Models\nwith Search for Knowledge-intensive Tasks. In Proceedings of the ACM Web\nConference 2024 (WWW â€™24), May 13â€“17, 2024, Singapore, Singapore. ACM,\nNew York, NY, USA, 13 pages. https://doi.org/10.1145/3589334.3645363\n1\nINTRODUCTION\nLarge Language Models (LLMs) such as ChatGPT have shown\npromising performance in various natural language processing\ntasks [2, 47]. However, for the complex knowledge-intensive tasks\nthat require multi-step reasoning and each step needs knowledge to\nsolve [23, 43, 49], many studies have shown that LLMs have trouble\nin: (1) compositional reasoning over multiple knowledge [24], (2)\nmemorization of long-tail and real-time knowledge [13] and (3)\navoiding hallucination that is inconsistent with the facts [1], which\naffects the accuracy and credibility of LLMs for complex knowledge-\nintensive tasks. Besides, context-only generation without any sup-\nporting evidence causes less traceability and makes people less\ntrust in the LLM-generated content. Retrieval-augmented method\nhas good potential to solve these problems because it combines the\nknowledge of the model with external knowledge bases [10, 12, 18].\nHowever, where and how to introduce IR into LLM is not a trivial\nthing. There are three main challenges. C-1: Directly inserting IR\ninto the reasoning process of LLM such as Self-Ask [24], LTM [48],\nReact [42] and DSP [15] leads to breaking the reasoning chain of\nLLM. Because in these methods, LLM can only reason a local sub-\nquestion in each generation. C-2: When there is a conflict in the\nknowledge of IR and LLM, for the knowledge that the LLM has\ncorrectly memorized, it risks being misled by IR if IR retrieves the\narXiv:2304.14732v7  [cs.CL]  24 Feb 2024", "WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua\nQ\nA\nB\nC\nD\nQ\n    Tree-of-Reasoning (Trace.)\nFinal Content\nwith References\nA\nB\nC\nD\nE\nF\nH\nI\nJ\nK\nL\nM\nN\nG\nQ\nA\nB\nC\nD\nIR\nA\nB\nC\nD\nQ\nE\nF\nG\nLLM\nLLM\nA\nB\nC\nD\nQ\nE\nF\nG\nLLM\nA\nB\nC\nD\nQ\nE\nF\nG\nH\nI\nJ\nK\nLLM\nLLM\nL\nM\nN\nRound 4 (CoQ Gen.)\nRound 3 (Verif. and Comp.)\nRound 3 (CoQ Gen.)\nRound 2 (CoQ Gen.)\nRound 2 (Verif.)\nRound 1 (Verif.)\nRound 1 (CoQ Gen.)\nIR\nIR\nIR\nLLM\nA\nB\nC\nD\nQ\nE\nF\nG\nH\nI\nJ\nK\nLLM\nIR\nIR\nA\nB\nC\nD\nQ\nE\nF\nG\nH\nI\nJ\nK\nLLM\nL\nM\nN\nA\nB\nC\nD\nQ\nE\nF\nG\nH\nI\nJ\nK\nIR\nIR\nIR\nIR\nIR provides \ndocument \nand answer\nIR provides \ndocument \nand answer\nIR\nRound 4 (Verif.)\nIR provides \ndocument \nand answer\nThe performer of Spirit If... is Kevin Drew [1]. Kevin \nDrew was born in Toronto [2]. Greyhound buses in \nToronto leave from Toronto Coach Terminal [3]. So the \nfinal answer is Toronto Coach Terminal.\n[1] Spirit If... is the debut solo album by Kevin Drew. It \nwas released on September 18, 2007 â€¦\n[2] Kevin Drew (born September 9, 1976 in Toronto) ..\n[3] The Toronto Coach Terminal is the central bus \nstation for inter-city services in Toronto, Ontario, \nCanada â€¦ when it was leased out in its entirety to bus \nlines Coach Canada and Greyhound Canada â€¦\nWhere do greyhound buses that are in \nthe birthplace of Spirit If...'s performer \nleave from? \n: Completion (need additional knowledge)\n: Verification (need to be corrected)\n: Verification (do not need to be corrected)\nQ\nCoQ 1\nCoQ 2\nCoQ 3\nCoQ 4\nFigure 1: Interaction between IR and LLM in SearChain. First, SearChain makes LLM plan a CoQ where each node is a query-\nanswer pair. Then, IR interacts with each node of CoQ to perform verification and completion. If IR detects that a node needs to\nbe corrected or provided with knowledge, it gives feedback to LLM and LLM re-generates a new CoQ, which is the new branch\nof the tree. This process is the node-identify Depth-first Search on a tree called Tree-of-Reasoning (the correct reasoning path\nis green). The final content includes the reasoning process and references to supporting documents.\nwrong information. It is important to make sure that IR only pro-\nvides the knowledge that LLM really needs. C-3: Previous methods\ncannot dynamically modify the reasoning direction.\nIn this paper, we propose a novel framework named Search-\nin-the-Chain (SearChain) to effectively combine LLM with IR to\nsolve the above challenges (Figure 1). SearChain and previous meth-\nods both need multiple IR-LLM interaction rounds, but the former\nworks at the chain level, while the latter only deals with a node. In\neach round, SearChain performs reasoning, verification, and com-\npletion. After the interaction, SearChain performs tracing to gener-\nate the final content. Specifically, in each round, first, LLM exploits\nin-context learning to construct a Chain-of-Query (CoQ), which\nis a reasoning chain to decompose and solve complex questions.\nEach node of the chain consists of an IR-oriented query, the answer\ngenerated by LLM for this query, and a flag indicating whether\nLLM needs additional knowledge. Different from previous methods\nin which LLM can only perform one-step reasoning (only a node)\nwhen interacting with IR, CoQ is a complete chain. This design\navoids IR from breaking the reasoning chain (C-1). Second, IR inter-\nacts with each node of CoQ to perform verification and completion.\nIn verification, IR verifies the answer of each node. In case when the\nLLM-generated answer is not consistent with the retrieved informa-\ntion and IR gives high confidence, IR gives feedback to LLM to help\nit correct the answer and re-generate the correct CoQ. In comple-\ntion, IR determines whether the node has missing knowledge from\nthe flag of the node and provides this knowledge to LLM to help it\nre-generate CoQ. LLM gradually generates the correct CoQ through\nmultiple rounds of interaction with IR. The above design provides\nLLM with the knowledge it really needs to alleviate the misleading\ncaused by IR to LLM (C-2), which improves accuracy. IR verifies\nand corrects the knowledge in the reasoning process of LLM based\non external knowledge bases, which improves credibility. After the\ninteraction, SearChain performs tracing to generate the reasoning\nprocess and marks references to supporting documents for each\nreasoning step, which is used as the final content returned to the\nuser. This improves the traceability of knowledge in the generated\ncontent. Interaction with IR in SearChain transforms the reasoning\npath from a chain to node-identify Depth-first Search on a tree\ncalled Tree-of-Reasoning (ToR). CoQ generation can be seen as a\npart of Depth-first Search and IR can identify the nodes that need\nmore information (C-3). This enables LLM to dynamically modify\nthe reasoning direction. This paperâ€™s main contributions are:\n(1) We highlight the challenges in introducing IR into LLM from\nthe perspectives of reasoning and knowledge.\n(2) SearChain not only improves the knowledge-reasoning ability\nof LLM but also uses IR to identify and give the knowledge that LLM\nreally needs. Besides, SearChain can mark references to supporting\ndocuments for the knowledge involved in the generated content.\n(3) Interaction with IR in SearChain forms a novel reasoning\npath: node-identify Depth-first Search on a tree, which enables\nLLM to dynamically modify the direction of reasoning.\n(4) Experiment shows that SearChain outperforms state-of-the-\nart baselines on complex knowledge-intensive tasks including multi-\nhop Q&A, slot filling, fact checking and long-form Q&A. Code is\nreleased at https://github.com/xsc1234/Search-in-the-Chain.\n2\nRELATED WORK\n2.1\nChain-of-Thought Prompting\nChain-of-thought [36] proposes the method that uses few-shot ex-\namples to enable LLM to give intermediate reasoning results to\nimprove the reasoning ability. [16] uses \"Letâ€™s do it step by step\" as\nprompt to achieve promising zero-shot performance. Auto-CoT ex-\nploits language models to automatically construct few-shot learning\nexamples for CoT [45]. There are also many studies that cover other\naspects of CoT such as self-consistency [35], usage of small and", "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nmedium size models [44] and selection [8]. Besides, there are stud-\nies that iteratively use LLM to decompose complex questions and\nanswer sub-questions step by step. These methods include Least-to-\nMost [48], Dynamic Least-to-Most [5], Self-Ask [24] and DSP [15].\nChain-of-Query of our method is also inspired by CoT. However,\nprevious studies focus on giving intermediate reasoning results or\ndecomposing complex questions and answering sub-questions step\nby step. They focus on how to solve local sub-questions while ignor-\ning the global planning of the reasoning chain. Although AgentGPT\nand PS [34] first plan each sub-question and then solve them, they\nare not suitable for scenarios where the next sub-question needs the\nanswer of the previous sub-questions to generate, which is common\nfor complex knowledge-intensive tasks (multi-hop QA). CoQ of our\nmethod makes LLM construct a global reasoning chain where each\nnode is a query-answer pair. This design not only improves the\nknowledge-reasoning ability but also provides the interface for IR\nto be deeply involved in the reasoning process of LLM.\n2.2\nRetrieval-augmented Language Models\nMany studies have shown that retrieval-augmented methods get\npromising performance in various natural language tasks such as\nopen-domain question answering [4, 10, 12, 18, 21, 37, 39], language\nmodeling [3, 20, 22] and enhancing the factuality [25]. Recently,\nsome studies enable LLM to interact with IR via in-context learn-\ning [15, 24, 28, 42]. In these methods, the interaction between IR and\nLLM makes the reasoning of LLM not continuous. LLM can only\nperform one-step reasoning at each inference. Our method makes\nLLM generate a global reasoning chain called Chain-of-Query at\neach inference, which introduces stronger logical relationship be-\ntween each reasoning step. Besides, previous methods can only\nprovide information to the LLM but cannot assist LLM in correct-\ning erroneous information or avoid the negative effect of IR on LLM,\nwhich makes the reasoning of LLM still in a one-dimensional chain.\nOur method makes IR interact with each node of the chain. IR only\nprovides LLM with its missing knowledge and corrects the answers\nthat are not consistent with the retrieved information when IR is\nconfident enough. This mitigates the negative effect of IR on LLM\nand transforms the reasoning path from chain to node-identify\nDepth First Search on a tree to enable LLM to dynamically modify\nthe reasoning direction.\n3\nOUR METHOD\nThis section introduces the design of Search-in-the-Chain (SearChain).\nIn SearChain, IR and LLM conduct multiple rounds of interaction.\nIn each round, first, LLM acts as the commander to plan the global\nreasoning chain for the complex input questions called Chain-of-\nQuery (CoQ). Each node of the CoQ consists of an IR-oriented\nquery, the answer for this query, and a flag indicating whether LLM\nneeds additional knowledge. Then, IR interacts with each node of\nCoQ and performs the completion and verification to only provide\nLLM with missing knowledge and correct the wrong answers to\nalleviate the misleading. LLM re-generates new CoQ based on feed-\nback from IR. Multiple rounds of interaction help LLM gradually\ngenerate the correct CoQ, which improves accuracy and credibility.\nFinally, SearChain performs tracing to generate the whole reason-\ning process and marks references to supporting documents for each\nğ‘„ğ‘„\nA\nLLM\nIR\nB\nLLM\nIR\nLLM\nIR\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nL\nM\nN\nK\nFinal Content\nwith References\n...\nFinal Answer\nPreviousï¼š\nOursï¼š\nğ‘„ğ‘„\nlocal node\nlocal node\nFigure 2: Comparison with previous methods.\nreasoning step, which is used as the final content returned to the\nuser. This improves the traceability of generated content. Interac-\ntion with IR in SearChain transforms the reasoning path from a\nchain to node-identify Depth-first Search on a tree called Tree-of-\nReasoning (ToR), which enables LLM to dynamically modify the\nreasoning direction.\n3.1\nComparison with Previous Methods\nFigure 2 shows the difference between our method and previous\nretrieval-augmented methods (Self-Ask [24], React [42], DSP [15],\netc.) in solving complex knowledge-intensive questions.\n(1) Local vs. Global. For a complex question that needs multi-\nstep reasoning, previous methods directly insert IR into the multi-\nstep reasoning process, causing LLM can only reason a local sub-\nquestion such as node A\nâ—‹in each generation. This breaks the rea-\nsoning chain of LLM. Our method proposes Chain-of-Query to\nprovide the interactive interface for IR on the premise of ensuring\nthe coherence of reasoning chain (plans a global chain for question\nğ‘„such as A\nâ—‹â†’B\nâ—‹â†’C\nâ—‹â†’D\nâ—‹in each generation). (solves C-1)\n(2) Directly Provide vs. Verify and Complete. Previous methods\ndirectly provide the retrieved information to the LLM. When the\nretrieved information is incorrect, the LLM runs the risk of being\nmisled. In our method, IR only corrects inconsistent information\nin Chain-of-Query when IR is confident enough, and provides the\ninformation that LLM does not know via flags on Chain-of-Query,\nwhich mitigates the negative effect of IR on LLM. (solves C-2)\n(3) Chain vs. Tree. Previous methods cannot modify the reasoning\ndirection in time as necessary. Our method transforms the reasoning\npath from a chain to node-identify Depth-first Search on a tree by\nintroducing the verification and completion from IR, which enables\nLLM to dynamically modify the direction of reasoning. (solves C-3)", "WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua\nConstruct a global reasoning chain for this complex [Question] : Where do \nGreyhound buses that are in the birthplace of Spirit If...'s performer leave from? \nYou should generate a query to the search engine based on what you already \nknow at each step of the reasoning chain, starting with [Query]. \nIf you know the answer for [Query], generate it starting with [Answer].\nYou can try to generate the final answer for the [Question] by referring to the \n[Query]-[Answer] pairs, starting with [Final Content].\nIf you don't know the answer, generate a query to search engine based on what \nyou already know and do not know, starting with [Unsolved Query].\nFor example:\n[Question]: Where do Greyhound buses that are in the birthplace of Spirit If...'s \nperformer leave from? \n[Query 1]: Who is the performer of Spirit If... ?\nIf you don't know the answer:\n[Unsolved Query]: Who is the performer of Spirit If... ?\nIf you know the answer:\n[Answer 1]: The performer of Spirit Ifâ€¦ is Kevin Drew.\n[Query 2]: Where was Kevin Drew born?\nIf you don't know the answer:\n[Unsolved Query]: Where was Kevin Drew born?\nIf you know the answer:\n[Answer 2]: Toronto.\n[Query 3]: Where do greyhound buses in Toronto leave from?\nIf you don't know the answer:\n[Unsolved Query]: Where do greyhound buses in Toronto leave from?\nIf you know the answer:\n[Answer 3]: Toronto Coach Terminal.\n[Final Content]: The performer of Spirit Ifâ€¦ is Kevin Drew [1]. Kevin Drew \nwas born in Toronto [2]. Greyhound buses in Toronto leave from Toronto \nCoach Terminal [3]. So the final answer is Toronto Coach Terminal.\nNode 1\nNode 2\nNode 3\nFigure 3: Prompt to make LLM generate Chain-of-Query.\n3.2\nChain-of-Query Generation\nIn SearChain, we use in-context learning [36] to prompt large lan-\nguage model to construct a global reasoning chain for complex\nquestion ğ‘„named Chain-of-Query (CoQ):\nCoQ = (ğ‘1,ğ‘1) â†’(ğ‘2,ğ‘2) â†’... â†’(ğ‘ğ‘›,ğ‘ğ‘›),\n(1)\nwhich is the branch of Tree-of-Reasoning. Each node (ğ‘ğ‘–,ğ‘ğ‘–) of\nCoQ consists of an IR-oriented query ğ‘ğ‘–and its answer ğ‘ğ‘–. ğ‘1 ...\nğ‘ğ‘›are the sub-questions that need to be solved in the reasoning\nprocess of solving ğ‘„. CoQ generation is applied to each round of\ninteraction between LLM and IR. In the first round, the prompt\nused to make LLM generate CoQ is shown in Figure 3. The prompt\nwith â€œConstruct a global reasoning chainâ€ makes LLM know that the\nmain task is to generate a global reasoning chain in each genera-\ntion. \"Global\" means that LLM needs to plan a complete reasoning\nchain for the complex question, rather than answer the question\ndirectly or only solve \"local\" sub-questions (comparison shown in\nFigure 2). At each node of the chain, LLM focuses on generating the\nIR-oriented query and gives the answer if LLM knows. If LLM does\nnot know the answer, it should mark the query with â€œ[Unsolved\nQuery]â€, which is a flag indicating the missing of knowledge. In sub-\nsequent rounds, when a node needs IR to correct or provide missing\nknowledge, LLM generates a new CoQ according to the feedback\nof IR to dynamically modify the reasoning direction. The design for\nthis scenario will be introduced in Section 3.3. The generation of\nCoQ is a complete Depth-first Search for ğ‘„, which avoids IR from\nbreaking the reasoning chain of LLM. Experiments (Section 4.3.3)\nalso show that for the difficult sub-question, CoQ enables LLM to\nAlgorithm 1: Description of the Interaction with IR.\nInitialize:Processed queries: ğ‘€= ğ‘›ğ‘¢ğ‘™ğ‘™;\nCorrect reasoning path: ğ‘…= ğ‘›ğ‘¢ğ‘™ğ‘™;\nInteraction rounds: ğ‘Ÿ= 0;\nFeedback: ğ¹= ğ‘›ğ‘¢ğ‘™ğ‘™; ToR: ğ‘»= ğ‘„;\nFunction IR(ğ‘ğ‘–, ğ‘ğ‘–):\nğ‘‘ğ‘–= Retrieval(ğ‘ğ‘–); // Retrieve Top-1 document ğ‘‘ğ‘–for ğ‘ğ‘–.\nğ‘”, ğ‘“= Reader(ğ‘ğ‘–, ğ‘‘ğ‘–);\n// Extract answer ğ‘”from ğ‘‘ğ‘–and give confidence ğ‘“.\nif ğ‘ğ‘–is Unsolved Query then\n// Completion.\nğ‘….add (ğ‘ğ‘–, ğ‘”, ğ‘‘ğ‘–);\n// Record the correct node.\nreturn PromptForComplete(ğ‘ğ‘–, ğ‘”, ğ‘‘ğ‘–);\nif ğ‘“> ğœƒand NotEqual(ğ‘”,ğ‘ğ‘–) then\n// Verification.\nğ‘….add(ğ‘ğ‘–, ğ‘”, ğ‘‘ğ‘–);\n// Record the correct node.\nreturn PromptForVerify(ğ‘ğ‘–, ğ‘”, ğ‘‘ğ‘–);\nğ‘….add (ğ‘ğ‘–, ğ‘ğ‘–, ğ‘‘ğ‘–); return â€œPassâ€ ;\nFunction Traverse(ğ‘ªğ’ğ‘¸):\nforeach (ğ‘ğ‘–,ğ‘ğ‘–) in ğ‘ªğ’ğ‘¸do\nif not DuplicateQuery(ğ‘ğ‘–, ğ‘€) then\n// If ğ‘ğ‘–has not been processed.\nğ¹= IR(ğ‘ğ‘–, ğ‘ğ‘–); ğ‘€.add(ğ‘ğ‘–);\nif not F == â€œğ‘ƒğ‘ğ‘ ğ‘ â€ then return ğ¹;\nreturn â€œğ¹ğ‘–ğ‘›ğ‘–ğ‘ â„â€ ;\nFunction Main(ğ‘„,ğ¹):\nwhile not (F == â€œFinishâ€ or ğ‘Ÿ> ğ‘Ÿğ‘šğ‘ğ‘¥) do\nğ‘ªğ’ğ‘¸= ChainGenerate(ğ‘„, ğ¹);\n// LLM generate the new Chain-of-Query ğ‘ªğ’ğ‘¸.\nğ‘».AddChild(ğ‘ªğ’ğ‘¸); // Add the new branch to ğ‘».\nğ¹= Traverse(ğ‘ªğ’ğ‘¸); // Interact with IR.\nğ‘Ÿ= ğ‘Ÿ+ 1; // Update the number of interaction rounds ğ‘Ÿ.\nreturn Tracing(ğ‘», ğ‘…)\nsolve it by more reasoning steps such as rewriting or further de-\ncomposing the sub-question while baselines tend to stop reasoning.\nIt is because baselines focus on solving current local sub-questions\nwhile ignoring the global planning of the reasoning chain. The\nglobal perspective in CoQ makes LLM try more to explore possible\nanswers when facing intermediate difficulties.\n3.3\nInteraction with Information Retrieval\nIn each round of interaction, LLM passes the generated CoQ to\nIR. IR verifies and completes the information for each node (ğ‘ğ‘–,ğ‘ğ‘–)\nof CoQ and feeds back to LLM to help it generate more correct\nCoQ as the new branch of ToR (Tree-of-Reasoning). Besides, IR\nrecords the corresponding retrieved documents for each node of\nCoQ as its supporting documents, which enhances the traceabil-\nity of LLM-generated content. The description of interaction is\nshown in Algorithm 1. IR interacts with each node (ğ‘ğ‘–,ğ‘ğ‘–) of CoQ,\nretrieves the Top-1 document ğ‘‘ğ‘–for ğ‘ğ‘–as the supporting document,\nand judges whether to verify or complete it according to the type of\nğ‘ğ‘–. When all the queries of CoQ do not need to be corrected or com-\npleted, or the maximum number of interaction rounds is reached,", "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nthe interaction ends. SearChain traces back the correct reasoning\npath of ToR and refers to each node of the path to generate the\nfinal content with marked references to supporting documents for\nknowledge of each node.\nVerification. Verification aims to guarantee the correctness of\nğ‘ğ‘–in each node (ğ‘ğ‘–,ğ‘ğ‘–) of CoQ based on the external knowledge\nbase, which improves the accuracy and credibility of generated\ncontent. Specifically, given the retrieved Top-1 document ğ‘‘ğ‘–for ğ‘ğ‘–, a\nReader [14] that has been trained on open-domain QA datasets [14]\nis used to extract the answer ğ‘”for ğ‘ğ‘–from ğ‘‘ğ‘–with its confidence ğ‘“\n(ğ‘“is a predicted value that measures whether ğ‘”can answer ğ‘ğ‘–):\nğ‘ = arg max(softmax(Hwğ‘ )),ğ‘’= arg max(softmax(Hwğ‘’)),\nğ‘”= ğ‘‘ğ‘–[ğ‘ : ğ‘’], ğ‘“= H[ğ¶ğ¿ğ‘†]wğ‘“, (wğ‘ , wğ‘¡, wğ‘“âˆˆRğ¸),\nwhere H âˆˆRğ¿Ã—ğ¸is the sequence of last hidden states for the input\ntext â€œ[CLS]ğ‘ğ‘–[SEP]ğ‘‘ğ‘–â€, ğ¿is the length and ğ¸is hidden dimension.\nH[CLS] is the last hidden state of [CLS] token. Then, SearChain\njudges whether the answer ğ‘ğ‘–given by LLM is consistent with\nthe retrieved information according to (1) whether ğ‘”appears in\nğ‘ğ‘–(for short-form generation tasks such as multi-hop QA and slot\nfilling) or (2) whether ROUGE [19] between ğ‘ğ‘–andğ‘‘ğ‘–is greater than\nthe threshold ğ›¼(for long and free-form generation tasks such as\nELI5 [7]). If ğ‘ğ‘–is not consistent with retrieved information and the\nReader is confident enough (ğ‘“> ğœƒ, ğœƒis a threshold to alleviate the\nnegative effect of IR on LLM), a prompt is constructed to help LLM\ncorrect the answer ğ‘ğ‘–. The template of the prompt is: â€œAccording\nto the Reference, the answer for ğ‘ğ‘–should be ğ‘”, you can change your\nanswer and continue constructing the reasoning chain for [Question]:\nğ‘„. Reference: ğ‘‘ğ‘–.â€. This round is over. LLM receives the feedback of\nIR, gives the new answer ğ‘â€²\nğ‘–for ğ‘, and generates a new CoQ with\n(ğ‘ğ‘–,ğ‘â€²\nğ‘–) as the root node, which is the new branch of ToR.\nCompletion. Completion aims to provide LLM with missing\nknowledge in nodes of CoQ, which improves the accuracy of gen-\nerated content. Specifically, in CoQ generation (Section 3.2), LLM\nmarks â€œ[Unsolved Query]â€ for the unsolvable query. For the unsolv-\nable query ğ‘âˆ—\nğ‘–, IR extracts the answer ğ‘”âˆ—from retrieved document\nğ‘‘âˆ—\nğ‘–as described in Verification. Regardless of whether ğ‘“is greater\nthan the threshold ğœƒ, ğ‘”âˆ—and ğ‘‘âˆ—\nğ‘–will be fed back to the LLM in the\nform of a prompt because the LLM cannot solve ğ‘âˆ—\nğ‘–. The template of\nthe prompt is: â€œAccording to the Reference, the answer for ğ‘âˆ—\nğ‘–should\nbe ğ‘”âˆ—, you can give your answer and continue constructing the rea-\nsoning chain for [Question]: ğ‘„. Reference: ğ‘‘âˆ—\nğ‘–.â€. This round is over.\nLLM receives the feedback, gives the answer ğ‘âˆ—\nğ‘–to solve the query\nğ‘âˆ—\nğ‘–and generates a new CoQ with (ğ‘âˆ—\nğ‘–, ğ‘âˆ—\nğ‘–) as the root node, which\nis the new branch of ToR.\nTracing. Tracing aims to generate the reasoning process and\nmark references to supporting documents for each reasoning step,\nwhich is used as the final content returned to the user. This im-\nproves the traceability of each knowledge in the generated content.\nSpecifically, SearChain records the documents retrieved for each\nnode on the correct reasoning path of Tree-of-Reasoning as the\nsupporting documents. SearChain prompts LLM to generate the\nfinal content by referring to nodes on the correct path and marking\nreferences to the supporting documents for the corresponding sub-\nfragments of the generated content (final content of Figure 1). The\nprompt is â€œYou can try to generate the final answer for the [Ques-\ntion] by referring to the [Query]-[Answer] pairs, starting with [Final\nContent]. [Query 1]: ğ‘1. [Answer 1]: ğ‘1 ... [Query ğ‘š]: ğ‘ğ‘š. [Answer ğ‘š]:\nğ‘ğ‘š.â€. This design enables the user to acquire the related documents\nof the knowledge involved in each step of reasoning. We believe\nthat it is a promising task to mark references to supporting doc-\numents on sub-fragments of complex content generated by LLM.\nOur approach provides a novel and effective approach to solve this\ntask by retrieving supporting documents for each sub-questions\ninvolved in the reasoning process of LLM without any supervised\ndata (texts with citation annotations) and training of the LLM.\nNode-identify Depth-first Search. Compared with previous\nretrieval-augmented methods, interaction with IR in SearChain\nforms a novel reasoning path: node-identify Depth-first Search\non a tree. In each generation, LLM generates a CoQ to perform\ncontinuous reasoning on complex questions until the final answer\nis generated or an unsolvable sub-question is encountered. This can\nbe seen as a part of Depth-first Search (DFS). However, different\nfrom traditional DFS algorithm [31], \"node-identify\" in SearChain\nmeans that when a search in one direction is terminated, SearChain\ndoes not return to its parent node, but dynamically identifies the\nnode that needs to be corrected or completed via verification and\ncompletion in IR and re-generates a new CoQ started with this\nnode. The interaction process between IR and LLM in SearChain is\nthe process of constructing a tree using node-identify DFS, which\nenables LLM to dynamically modify the reasoning direction.\n4\nEXPERIMENTS\n4.1\nExperimental Setup\n4.1.1\nDatasets and Evaluation Metric. We select four classic\ncomplex knowledge-intensive tasks including multi-hop question-\nanswering (HotpotQA (HoPo) [40], Musique (MQ) [33], WikiMulti-\nHopQA (WQA) [11] and StrategyQA (SQA) [9]), slot filling (zsRE [17],\nT-REx [6]), fact checking (FEVER [32]) and long-form question-\nanswering (ELI5 [7]). These tasks require LLM to perform multi-\nstep reasoning on complex questions, and each step requires cor-\nresponding knowledge to solve. As for the evaluation metrics, for\nELI5 whose ground truth is long and free-form, we use ROUGE-\nL [19] as the metric. For other tasks, we use whether the ground\ntruth answer is contained within the generated answer (i.e, cover-\nEM [26]) as the metric. Following DSP [15] and Self-Ask [24], we\nevaluate the model on full development datasets of MQ and HoPo,\nBIG-bench [29] datasets on SQA and subsets of WQA, zsRE, T-REx,\nFEVER and ELI5 (each subset has 1.2k questions).\n4.1.2\nBaselines. Our baselines can be divided into two categories,\none is about improving the reasoning ability of LLM on complex\ntasks (CoT [36], CoT-SC [35], Auto-CoT [45], Recite-and-answer [30]\nand Least-to-Most [48]), and the other is not only introducing IR\nto LLM but also improving the reasoning ability (Direct1, Self-\nAsk [24], ToolFormer2 [28], React [42], DSP [15], Verify-and-Edit\n(combined with CoT-SC) [46] and Tree-of-Thought [41]). AgentGPT\nand PS [34] use Plan-and-Solve paradigm, we also reproduce this\nas one of the baselines.\n1Retrieve documents and provide them to LLM in a prompt.\n2Perform ToolFormer on gpt-3.5-turbo via in-context learning.", "WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua\nTable 1: Performance of SearChain and baselines on complex knowledge-intensive tasks. Bold denotes the best result in\ndifferent settings. FC: Fact Checking, LFQA: Long-Form QA. Metric for LFQA: ROUGE-L. Metric for others: cover-EM.\nMuti-Hop QA\nSlot Filling\nFC\nLFQA\nHoPo\nMQ\nWQA\nSQA\nzsRE\nT-REx\nFEV.\nELI5\nWithout Information Retrieval\nDirect Prompting\n31.95\n5.91\n25.82\n66.25\n22.75\n43.85\n73.45\n21.90\nAuto-CoT\n33.53\n10.55\n29.15\n65.40\n21.30\n43.98\n76.61\n21.55\nCoT\n35.04\n9.46\n30.41\n65.83\n22.36\n44.51\n76.98\n21.79\nCoT-SC\n36.85\n10.02\n32.68\n70.84\n24.74\n46.06\n77.15\n22.05\nRecite-and-answer\n36.49\n10.97\n32.53\n70.47\n24.98\n46.14\n77.35\n22.10\nSelf-Ask w/o IR\n33.95\n11.10\n35.65\n65.45\n20.16\n44.71\n75.31\n21.73\nLeast-to-Most\n34.05\n11.45\n32.88\n65.78\n21.86\n44.98\n75.98\n21.95\nPlan-and-Solve\n36.33\n12.95\n35.68\n73.21\n25.15\n47.58\n77.08\n22.23\nSearChain w/o IR\n38.36\n13.61\n40.49\n75.62\n30.14\n52.69\n77.06\n22.54\nInteraction with Information Retrieval\nDirect Retrieval\n34.09\n10.22\n30.01\n66.78\n52.29\n59.28\n78.25\n23.40\nToolFormer\n36.75\n12.98\n35.49\n67.02\n51.35\n59.17\n80.79\n23.05\nSelf-Ask\n40.05\n14.28\n39.58\n67.65\n50.51\n59.12\n79.41\n23.25\nPlan-and-Solve w/ IR\n41.65\n15.07\n42.05\n74.58\n52.15\n60.03\n81.04\n24.56\nReact â†’CoT-SC\n43.15\n15.49\n40.36\n70.43\n53.27\n60.42\n80.59\n24.05\nVerify-and-Edit\n44.03\n15.57\n40.83\n71.09\n53.95\n61.10\n80.67\n23.80\nTree-of-Thought w/ IR\n50.65\n15.61\n42.49\n72.55\n54.88\n62.40\n81.03\n24.20\nDSP\n51.97\n15.83\n43.52\n72.41\n54.35\n61.32\n80.65\n23.46\nSearChain\n56.91\n17.07\n46.27\n76.95\n57.29\n65.07\n81.15\n25.57\n- w/o Verification\n46.11\n14.70\n42.67\n75.98\n43.58\n55.46\n78.79\n22.98\n- w/o Completion\n53.05\n15.86\n43.64\n76.53\n45.78\n56.03\n80.03\n25.02\n4.1.3\nImplementation. The large language model we used is gpt-\n3.5-turbo provided from API of OpenAI3 and the retrieval model\nwe used is ColBERTv2 [27] (following DSP). IR model infers on\none Tesla V100 GPU. For HotpotQA, we use Wikipedia 2017 as\nthe corpus, which is provided by [40] in full-wiki setting. For the\nother datasets, we use the large-scale passage collection built on\nWikipedia as the corpus [14, 38]. Baselines with information re-\ntrieval are in the same setting as SearChain. We reproduce all base-\nlines on gpt-3.5-turbo following the settings in their papers. The\nmaximum number of interaction rounds ğ‘Ÿğ‘šğ‘ğ‘¥is 5. The thresholds\nğ›¼and ğœƒare set as 0.35 and 1.5 respectively. As for the selection\nof confidence threshold (ğœƒ), we initialize the initial value of the\nconfidence threshold (1.0) based on prior knowledge and gradually\nincrease the value with a step size of 0.1. We validate the F1-score\non the mixed open-domain QA datasets (NQ, TriviaQA, WebQ, and\nTREC) after each value change. We find that when the confidence\nthreshold is 1.5, the highest F1-score can be achieved so we set\nthe confidence threshold as 1.5. As for the selection of ROUGE\nthreshold (ğ›¼), we determine this value by observing the ROUGE\nrelationship between the generated text and the ground truth in\nthe few examples in in-context learning. Our further experiments\nshow that when the value range of ROUGE threshold is between\n0.3 and 0.5, the performance change on ELI5 is not obvious. De-\ntails of prompts and experiments are introduced in Section A.3 of\nAppendix.\n3https://openai.com/api/\n4.2\nMain Results\nPerformance on knowledge-intensive tasks is shown in Table 1.\n(1) Effect of Chain-of-Query. CoQ is the reasoning chain for\ncomplex questions in SearChain. We compare it with recent com-\npetitive baselines in the setting without IR. SearChain w/o IR out-\nperforms all baselines based on CoT (CoT, Auto-CoT, CoT-SC and\nRecite-and-answer), which indicates that focusing on construct-\ning a global reasoning chain consisting of sub-questions is better\nthan just giving intermediate reasoning results. SearChain w/o IR\noutperforms Self-Ask w/o IR and Least-to-Most, which indicates\nthat it is more effective to focus on constructing a global reasoning\nchain at each inference (global perspective) than generating and\nanswering sub-questions step by step (local perspective).\n(2) Effect of interaction with IR. In the setting with interaction\nwith IR, SearChain again outperforms all the baselines. The para-\ndigm of first generating global CoQ, and then IR interacting with\neach node of CoQ ensures the coherence of LLM reasoning. This\nsolves the problem in Self-Ask, DSP and React. Besides, SearChain\ndecouples the knowledge of LLM and IR. IR judges whether to\nprovide information to LLM according to the confidence and the\nflag of the node on CoQ, which effectively alleviates misleading\nLLM. Last but not least, baselines reason in the one-dimensional\nchain. They cannot dynamically modify the reasoning direction.\nInteraction with IR in SearChain transforms the reasoning path\nfrom a chain to node-identify Depth-first Search on a tree, which\nenables LLM to dynamically modify the reasoning direction.", "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nTable 2: Distribution of knowledge sources.\nKnowledge Src.\nHoPo\nMQ\nWQA\nSQA\nLLM\n74.56%\n78.83%\n75.83%\n94.98%\nCorrected by IR\n20.94%\n14.60%\n18.96%\n2.78%\nCompleted by IR\n4.50%\n6.57%\n5.21%\n2.24%\nTable 3: Positive and negative effects of IR on LLM.\n(a) Accuracy on Sğ¼ğ‘…and S (positive effect â†‘).\nHoPo\nMQ\nWQA\nSQA\nw/o IR (S)\n38.36\n13.61\n40.49\n75.62\nw/o IR (Sğ¼ğ‘…)\n31.38\n10.20\n32.60\n68.96\nw IR (Sğ¼ğ‘…)\n60.86\n18.49\n50.52\n78.42\n(b) Percentage that IR misleads LLM (negative effect â†“).\nHoPo\nMQ\nWQA\nSQA\nSelf-Ask\n15.76\n14.32\n25.76\n10.29\nReact\n17.68\n15.22\n25.99\n10.03\nPlan-and-Solve w/ IR\n16.42\n15.25\n22.31\n7.59\nVerify-and-Edit\n9.78\n10.75\n16.44\n6.52\nTree-of-Thought w/ IR\n12.07\n13.25\n20.52\n8.46\nDSP\n14.72\n14.03\n24.31\n9.22\nSearChain\n6.33\n6.50\n12.71\n5.31\n4.3\nAnalysis\nIn this section, we discuss and demonstrate the advantages of\nSearChain compared to baselines in detail. First, we analyze the\nsource of the knowledge of SearChain in solving complex ques-\ntions. Second, while we analyze the positive effect of IR on LLM in\nsolving difficult questions, we also demonstrate that our method\ncan better mitigate the negative effect of IR on LLM. Third, we\nshow the advantages of SearChain compared to baselines in terms\nof reasoning and tracing capabilities. Last but not least, we perform\nefficiency analysis to show our method significantly improves task\nperformance with no significant increase in time consumption.\n4.3.1\nKnowledge Decoupling. We analyze the knowledge sources\non the four multi-hop QA datasets. Specifically, we classify knowl-\nedge sources into three categories: (1) knowledge of LLM, (2) knowl-\nedge that corrected by IR in verification, and (3) knowledge that\nLLM does not know and is provided by IR in completion. We use\nnode of ToR as the statistical granularity to calculate the percentage\nof nodes from these three sources in the total nodes respectively.\nThe experimental results are shown in Table 2. It is worth noting\nthat even though most of the knowledge comes from LLM, this\nknowledge is also verified by IR. IR only corrects the inconsistent\nanswer given by LLM when it is confident enough and provides\nLLM with the missing knowledge, which alleviates the negative\neffect of IR on LLM and improves the utilization of retrieved infor-\nmation. On StrategyQA, LLM has memorized most knowledge that\nIR can retrieve, so IR provides less knowledge than other datasets.\nTable 4: Number of reasoning steps. SearChain tries more for\nunsolvable sub-questions to achieve better accuracy.\n2-hop\n3-hop\n4-hop\nAccuracy\nCoT\n2.25\n2.23\n2.19\n9.46\nSelf-Ask w/o IR\n2.04\n2.21\n2.15\n11.10\nLeast-to-Most\n2.52\n2.68\n2.70\n11.45\nSearChain w/o IR\n4.16\n4.66\n5.06\n13.61\nIt is not clear what entity Robert J Chassell is a member of. The question cannot be answered.\nCoT\nSelf-Ask w/o Retrieval\nFollow up: Who is Robert J Chassell? \nIntermediate answer: Robert J Chassell is a programmer and author.\nFollow up: What organization is Robert J Chassell a member of? \nIntermediate answer: The organization that Robert J Chassell is a member of is not specified.\nSearChain w/o Retrieval\n[Query 1]: What is Robert J Chassell a member of?\n[Answer 1]: There is no information about what organization or group Robert J Chassell is a member of.\n[Query 2]: Can we find any information about Robert J Chassell's affiliations or associations?\n[Answer 2]: Yes, Robert J Chassell is identified as a member of the Free Software Foundation.\n[Query 3]: Who founded the Free Software Foundation?\n[Answer 3]: The Free Software Foundation was founded by Richard Stallman.\n[Final Content]: Richard Stallman founded the Free Software Foundation [3], which Robert J Chassell is \na member of [2].  So the final answer is Richard Stallman. \nWho founded what Robert J Chassell is a member of?\nFigure 4: Case study of the difference between SearChain and\nbaselines for unsolvable sub-questions.\n4.3.2\nPositive and Negative Effects of IR on LLM. (1) Positive.\nIn SearChain, IR can identify the trouble of LLM and effectively\nhelp LLM to correct the answers and acquire missing knowledge.\nWe select the questions (Sğ¼ğ‘…) that IR helps to correct or provide\nknowledge from the datasets used in Table 1 (S) and evaluate the\naccuracy of SearChain on Sğ¼ğ‘…. We also evaluate the accuracy of\nSearChain w/o IR on Sğ¼ğ‘…. The results in Table 3(a) show that w/o\nIR performs worse on Sğ¼ğ‘…than on S, which indicates that LLM\ndoes have trouble with the questions that require IR help. w/ IR\nperforms better on Sğ¼ğ‘…, which indicates that IR effectively identifies\nand solves the trouble of LLM. (2) Negative. Section 1 points out the\nrisk of IR misleading LLM when there is a conflict in the knowledge\nof IR and LLM. We select the questions (Sğ‘¡) that LLM can give\nthe correct answers to and count the percentage that LLM gives\nincorrect answers after adding IR on Sğ‘¡. Table 3(b) shows SearChain\neffectively mitigates the negative effect of IR on LLM. SearChain\nuses the confidence of IR and the information of CoQ to judge\nwhether to correct LLM or provide LLM with its missing knowledge.\n4.3.3\nCoQ vs Baselines in Reasoning. CoQ performs better\non reasoning complex questions than the baselines. In addition to\nTable 1, we further analyze the reasoning ability from two aspects:\n(1) Number of Reasoning Steps. We analyze the number of\nreasoning steps in different methods in the setting without IR. We\nconduct the experiment on Musique because Musique has more\ncomplex questions. Table 4 shows the average number of reason-\ning steps on questions with different hops. Our method has more\nreasoning steps, and the number of reasoning steps increases with\nthe hops of the question. This shows that our method has a better\nperception of the complexity of the questions.", "WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua\nNew Bing\nIâ€™m\nsorry,\nbut\nI\ncouldnâ€™t\nfind\nany\ninformation about a fungus described by\nElsie Maud Wakefield as â€œLepiotaâ€ in a\nLondon botanical garden. I did find that\nElsie Maud Wakefield was a mycologist at\nthe Royal Botanic Gardens, Kew from\n1910 to 1951 and was a specialist in\nBasidiomycetes\nand\nan\ninternational\nauthority on Aphyllophorales [1,2].\n[1]: â€¦ Elsie Maud Wakefield, was an\nEnglish\nmycologist\nand\nplant\npathologistâ€¦\n[2]: â€¦ Elsie Wakefield was a specialist in\nBasidiomycetes and an international auth-\nority on Aphyllophorales. â€¦\nSearChain\nRaymond Ochoa is an\nAmerican\nchild\nactor\nand voice actor [1]. He\nhas played various roles\nin\nmovies\nincluding\nArlo\nin\n\"The\nGood\nDinosaur\" [2]. Arlo is\nan Apatosaurus [3]. So\nthe\nfinal\nanswer\nis\nApatosaurus.\n[1]: Raymond Ochoa is\nan American child actor\nand voice actor â€¦\n[2]:Raymond Ochoa â€¦\nplayed,\nArlo,\nin\nthe\nPixar film \"The Good\nDinosaurâ€œ ...\n[3]\nâ€¦\ntimid\nApato-\nsaurus\nnamed\nArlo\n(Ochoa) â€¦\nSearChain\nJimmy Bartel was the reci-\npient of the 2007 Brown-low\nMedal\n[1]\nand\nplayed\nAustralian\nrules\nfootball\nprofessionally [2]. He played\nfor\nthe\nGeelong\nFootball\nClub\nin\nthe\nAustralian\nFootball League [2]. So the\nfinal\nanswer\nis\nGeelong\nFootball Club.\n[1]: â€¦ Jimmy Bartel won the\nmedal during the 2007 AFL\nseason â€¦\n[2]: â€¦ Jimmy Bartel is a\nformer\nAustralian\nrules\nfootballer\nplays\nfor\nthe\nGeelong Football Club ...\nWhat team did the recipient of\nthe 2007 Brownlow Medal play for? \nWhat kind of dinosaur does Raymond Ochoa \nvoice in the Pixar film \"The Good Dinosaur\"? \nNew Bing\nRaymond Ochoa vo-\nices the character of\nArlo, an Apatosaurus\nin the Pixar film â€œThe\nGood Dinosaurâ€ [1]\n[1]:(Amazon.com: The\nGood Dinosaur : Ray-\nmond Ochoa)\nSearChain\nElise\nMaud\nWakefield\nwas\nan\nEnglish\nmycologist\nand\nplant\npathologist\n[1].\n\"Lepiota\" is a genus of fungi [2]. Kew\nGardens is known for its fungi collection [3],\nsuch as \"Lepiota viridigleba\" [4]. So the final\nanswer is Kew Gardens.\n[1]: â€¦ Elsie Maud Wakefield, was an English\nmycologist and plant pathologistâ€¦\n[2]: â€¦ Lepiota viridigleba is a species of seq-\nuestrate fungus in the family Agaricaceae â€¦\n[3] â€¦ Kew Gardens is a botanical garden in\nsouthwest London that houses the \"largest and\nmost\ndiverse\nbotanical\nand\nmycological\ncollections in the world\". ..\n[4] â€¦ The collection of fungi at Kew Gardens\nincludes species such as â€¦ \"Lepiota viri-\ndiglebaâ€œ â€¦\nIn what London botanical garden can you find the \nfungus described by Elise Maud Wakefield as \"Lepiota\"?\nNew Bing\nThe recipient of the 2007\nBrownlow\nMedal\nwas\nJimmy\nBartel\nwho\nplayed for the Geelong\nFootball Club. He won\nthe\nmedal\nby\npolling\ntwenty-nine votes during\nthe 2007 AFL season [1].\nIs\nthere\nanything\nelse\nyou would like to know\nabout this topic?\n[1]:\nâ€¦\nJimmy\nBartel\nwon the medal by polling\ntwenty-nine votes during\nthe 2007 AFL seasonâ€¦\nFigure 5: Case study of SearChain and New Bing in marking references to supporting documents.\n(2) Solving Difficult Sub-questions. The baselines focus on\nsolving local sub-questions while ignoring the global planning of\nthe reasoning chain. This leads LLM to tend to stop reasoning rather\nthan try more when a sub-question cannot be solved. In our method,\nLLM acts as a commander that plans a global reasoning chain that\ncan solve the complex question, when a sub-question cannot be\nsolved, even without the help of IR, LLM can try to further decom-\npose or rewrite the sub-question to continue reasoning. It is because\nour method focuses on building a global chain that can solve the\ncomplex question (global perspective), rather than answering or\ngenerating the sub-questions step by step (local perspective). Case\nstudy in Figure 4 shows that CoT and Self-Ask stop the reasoning\nwhile SearChain continues reasoning by rewriting the sub-question.\nMore reasoning steps in Table 4 also support that SearChain can\ntry more for difficult sub-questions. More case studies are shown\nin Section A.1.1 of Appendix.\n4.3.4\nSearChain vs New Bing in Tracing. We compare the\nperformance of SearChain and New Bing in marking references for\ngenerated content via case study (Figure 5). We further propose two\nmetrics to evaluate the Scope of Knowledge Coverage and Accuracy\nof Marking Position to show traceability more intuitively:\nâ€¢ Scope of Knowledge Coverage (SKC) [0, +]: The number of\nknowledge items marked with supporting documents in the gener-\nated content. (statistics, SearChain (2.882) is better than New Bing\n(1.143))\nâ€¢ Accuracy of Marking Position (AMP) [0, 1]: The accuracy of\nthe position of the reference marks. That is, whether the references\nare correctly marked on the sub-fragments for the corresponding\nknowledge in the generated content. (human evaluation, SearChain\n(0.80) is better than New Bing (0.45))\nWe introduce three humans with masterâ€™s degrees to participate\nin our human evaluation and the results show that SearChain can\nmark references for each knowledge involved in the reasoning\nprocess (i.e., correct nodes of CoQ) in a fine-grained manner. While\nthe references given by New Bing do not cover all of the knowledge\nTable 5: Efficiency analysis.\n#ğ‘›â†“#ğ‘šâ†“#ğ‘Ÿâ†“ğ‘¡(ğ‘ ) â†“Perf. (Avg) â†‘\nSelf-Ask\n401\n63\n2.19 6.63\n46.73\nPlan-and-Solve w/ IR\n450\n71\n1\n6.05\n48.89\nReact â†’CoT-SC\n938\n110\n2.35 8.25\n48.47\nVerify-and-Edit\n565\n307\n2.40 13.90\n48.88\nTree-of-Thought w/ IR 622\n341\n2.29 13.28\n50.47\nDSP\n1759 155\n2.15 10.47\n50.44\nSearChain\n390\n189\n2.21 8.52\n53.29\nand cannot be marked on the correct position. More case studies\nare shown in Section A.1.2 of Appendix.\n4.3.5\nEfficiency Analysis. We analyze the running efficiency\nbetween SearChain and baselines on the number of words in the\ninput (ğ‘›) and output (ğ‘š) text of LLM, number of rounds of interac-\ntion between LLM and IR (ğ‘Ÿ) and overall running time (ğ‘¡). Table 5\nshows our method significantly improves task performance with\nno significant increase in time consumption. Most baselines also\nrequire multiple rounds of interaction between IR and LLM.\n5\nCONCLUSION\nIn this paper, we point out the challenges of introducing IR into\nLLM from the perspectives of reasoning and knowledge. We then\npropose a novel framework named SearChain to enable IR and LLM\nto interact with each other effectively. SearChain not only stimu-\nlates the knowledge-reasoning ability of LLM but also uses IR to\nprovide the knowledge that LLM really needs based on the external\nknowledge base, which improves both accuracy and credibility. Be-\nsides, SearChain can mark references to supporting documents for\nthe knowledge involved in the generated content, which improves\nthe traceability of the content. In addition, the interaction between\nIR and LLM in SearChain transforms the reasoning path from a\nchain to node-identify Depth-first Search on a tree, which enables", "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nLLM to dynamically modify the reasoning direction. Experimental\nresults on complex knowledge-intensive tasks show that SearChain\nperforms better than all baselines.\nACKNOWLEDGMENTS\nThis work was supported by the National Key R&D Program of\nChina (2022YFB3103700, 2022YFB3103704), the National Natural\nScience Foundation of China (NSFC) under Grants No. 62276248\nand U21B2046, and the Youth Innovation Promotion Association\nCAS under Grants No. 2023111.\nREFERENCES\n[1] Razvan Azamfirei, Sapna R Kudchadkar, and James Fackler. 2023. Large language\nmodels and the perils of their hallucinations. Critical Care 27, 1 (2023), 1â€“2.\n[2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan\nWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of\nChatGPT on Reasoning, Hallucination, and Interactivity. CoRR abs/2302.04023\n(2023). arXiv:2302.04023 https://doi.org/10.48550/arXiv.2302.04023\n[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, et al. 2022. Improv-\ning Language Models by Retrieving from Trillions of Tokens. In International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Mary-\nland, USA (Proceedings of Machine Learning Research, Vol. 162). PMLR, 2206â€“2240.\nhttps://proceedings.mlr.press/v162/borgeaud22a.html\n[4] Xue-Qi Cheng and Hua-Wei Shen. 2010. Uncovering the community structure\nassociated with the diffusion dynamics on networks. Journal of Statistical Me-\nchanics: Theory and Experiment 2010, 04 (2010), P04024.\n[5] Andrew Drozdov, Nathanael SchÃ¤rli, Ekin AkyÃ¼rek, Nathan Scales, Xinying Song,\nXinyun Chen, Olivier Bousquet, and Denny Zhou. 2023. Compositional Semantic\nParsing with Large Language Models. In The Eleventh International Conference\non Learning Representations. https://openreview.net/forum?id=gJW8hSGBys8\n[6] Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, et al. 2018. T-REx: A Large Scale\nAlignment of Natural Language with Knowledge Base Triples. In Proceedings of\nthe 2018 Conference on LREC. European Language Resources Association (ELRA),\nMiyazaki, Japan. https://aclanthology.org/L18-1544\n[7] Angela Fan, Yacine Jernite, Ethan Perez, et al. 2019. ELI5: Long Form Question\nAnswering. In Proceedings of the 2019 Conference on ACL. Association for Compu-\ntational Linguistics, Florence, Italy, 3558â€“3567. https://doi.org/10.18653/v1/P19-\n1346\n[8] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023.\nComplexity-Based Prompting for Multi-step Reasoning. In The Eleventh Interna-\ntional Conference on Learning Representations. https://openreview.net/forum?id=\nyf1icZHC-l9\n[9] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan\nBerant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with\nImplicit Reasoning Strategies. Transactions of the Association for Computational\nLinguistics (TACL) (2021).\n[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\n2020.\nREALM: Retrieval-Augmented Language Model Pre-Training.\nCoRR\nabs/2002.08909 (2020). arXiv:2002.08909 https://arxiv.org/abs/2002.08909\n[11] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.\nConstructing A Multi-hop QA Dataset for Comprehensive Evaluation of Rea-\nsoning Steps. In Proceedings of the 2020 Conference on COLING. International\nCommittee on Computational Linguistics, Barcelona, Spain (Online), 6609â€“6625.\nhttps://doi.org/10.18653/v1/2020.coling-main.580\n[12] Gautier Izacard and Edouard Grave. 2020. Leveraging Passage Retrieval with\nGenerative Models for Open Domain Question Answering. CoRR abs/2007.01282\n(2020). arXiv:2007.01282 https://arxiv.org/abs/2007.01282\n[13] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raf-\nfel. 2022.\nLarge Language Models Struggle to Learn Long-Tail Knowledge.\narXiv:2211.08411 [cs.CL]\n[14] Vladimir Karpukhin, Barlas Oguz, Sewon Min, et al. 2020. Dense Passage Retrieval\nfor Open-Domain Question Answering. In Proceedings of the 2020 Conference on\nEMNLP. Association for Computational Linguistics, Online, 6769â€“6781. https:\n//doi.org/10.18653/v1/2020.emnlp-main.550\n[15] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, et al. 2023. Demonstrate-Search-\nPredict: Composing retrieval and language models for knowledge-intensive NLP.\narXiv:2212.14024 [cs.CL]\n[16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwa-\nsawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in Neu-\nral Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (Eds.). https://openreview.net/forum?id=e2TBb5y0yFf\n[17] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-Shot\nRelation Extraction via Reading Comprehension. In Proceedings of the 2017 Confer-\nence on CoNLL). Association for Computational Linguistics, Vancouver, Canada,\n333â€“342. https://doi.org/10.18653/v1/K17-1034\n[18] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Genera-\ntion for Knowledge-Intensive NLP Tasks. In Proceedings of the 2020 Conference\non NeurIPS.\n[19] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out. Association for Computational Linguistics,\nBarcelona, Spain, 74â€“81. https://aclanthology.org/W04-1013\n[20] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Ha-\njishirzi, and Luke Zettlemoyer. 2022. Nonparametric Masked Language Mod-\neling. CoRR abs/2212.01349 (2022). https://doi.org/10.48550/arXiv.2212.01349\narXiv:2212.01349\n[21] Xiangyang Mou, Chenghao Yang, Mo Yu, Bingsheng Yao, Xiaoxiao Guo, Saloni\nPotdar, and Hui Su. 2021. Narrative Question Answering with Cutting-Edge\nOpen-Domain QA Techniques: A Comprehensive Study. Trans. Assoc. Comput.\nLinguistics 9 (2021), 1032â€“1046. https://doi.org/10.1162/tacl_a_00411\n[22] Shuzi Niu, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2012. Top-k learning to\nrank: labeling, ranking and evaluation. In Proceedings of the 35th international\nACM SIGIR conference on Research and development in information retrieval. 751â€“\n760.\n[23] Fabio Petroni, Aleksandra Piktus, Angela Fan, et al. 2021. KILT: a Benchmark for\nKnowledge Intensive Language Tasks. In Proceedings of the 2021 Conference on\nNAACL. Association for Computational Linguistics, Online, 2523â€“2544. https:\n//doi.org/10.18653/v1/2021.naacl-main.200\n[24] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike\nLewis. 2023. Measuring and Narrowing the Compositionality Gap in Language\nModels. https://openreview.net/forum?id=PUwbwZJz9dO\n[25] Hongjing Qian, Yutao Zhu, Zhicheng Dou, et al. 2023. WebBrain: Learning to\nGenerate Factually Correct Articles for Queries by Grounding on Large Web\nCorpus. arXiv:2304.04358 [cs.CL]\n[26] Corby Rosset, Chenyan Xiong, Minh Phan, et al. 2020.\nKnowledge-Aware\nLanguage Model Pretraining. CoRR abs/2007.00655 (2020). arXiv:2007.00655\nhttps://arxiv.org/abs/2007.00655\n[27] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei\nZaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late\nInteraction. In Proceedings of the 2022 Conference on NAACL. Association for\nComputational Linguistics, Seattle, United States, 3715â€“3734. https://doi.org/10.\n18653/v1/2022.naacl-main.272\n[28] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage Models Can Teach Themselves to Use Tools. arXiv:2302.04761 [cs.CL]\n[29] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb,\nAbubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta,\nAdriÃ  Garriga-Alonso, et al. 2022. Beyond the Imitation Game: Quantifying and\nextrapolating the capabilities of language models. arXiv:2206.04615 [cs.CL]\n[30] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.\nRecitation-Augmented Language Models. In The Eleventh International Conference\non Learning Representations. https://openreview.net/forum?id=-cqvvvb-NkI\n[31] Robert Tarjan. 1971. Depth-first search and linear graph algorithms. In 12th\nAnnual Symposium on Switching and Automata Theory (swat 1971). 114â€“121.\nhttps://doi.org/10.1109/SWAT.1971.10\n[32] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.\n2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In\nProceedings of the 2018 Conference on NAACL. Association for Computational\nLinguistics, New Orleans, Louisiana, 809â€“819. https://aclanthology.org/N18-1074\n[33] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.\n2022. MuSiQue: Multihop Questions via Single-hop Question Composition.\nTransactions of the Association for Computational Linguistics 10 (2022), 539â€“554.\nhttps://doi.org/10.1162/tacl_a_00475\n[34] Lei Wang, Wanyu Xu, Yihuai Lan, et al. 2023. Plan-and-Solve Prompting: Im-\nproving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.\narXiv:2305.04091 [cs.CL]\n[35] Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. 2023. Self-Consistency Im-\nproves Chain of Thought Reasoning in Language Models. In The Eleventh Inter-\nnational Conference on Learning Representations. https://openreview.net/forum?\nid=1PL1NIMMrw\n[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc\nLe, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in\nLarge Language Models. CoRR abs/2201.11903 (2022). arXiv:2201.11903 https:\n//arxiv.org/abs/2201.11903\n[37] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2022. Match-Prompt:\nImproving Multi-task Generalization Ability for Neural Text Matching via Prompt\nLearning. In Proceedings of the 31st ACM International Conference on Information\n& Knowledge Management. 2290â€“2300.", "WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua\n[38] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2023. BERM: Training\nthe Balanced and Extractable Representation for Matching to Improve General-\nization Ability of Dense Retrieval. arXiv preprint arXiv:2305.11052 (2023).\n[39] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-\naware Reranking-Truncation Joint Model for Search and Retrieval-augmented\nGeneration. arXiv preprint arXiv:2402.02764 (2024).\n[40] Zhilin Yang, Peng Qi, Saizheng Zhang, et al. 2018. HotpotQA: A Dataset for\nDiverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018\nConference on EMNLP. Association for Computational Linguistics, Brussels, Bel-\ngium, 2369â€“2380. https://doi.org/10.18653/v1/D18-1259\n[41] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao,\nand Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving\nwith Large Language Models. arXiv:2305.10601 [cs.CL]\n[42] Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. 2023. ReAct: Synergizing Reasoning and\nActing in Language Models. In The Eleventh International Conference on Learning\nRepresentations. https://openreview.net/forum?id=WE_vluYUL-X\n[43] Da Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei Chang, Furu Wei, and\nJianfeng Gao. 2022. A Survey of Knowledge-Intensive NLP with Pre-Trained\nLanguage Models. arXiv:2202.08772 [cs.CL]\n[44] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrap-\nping Reasoning With Reasoning. In Advances in Neural Information Processing\nSystems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n(Eds.). https://openreview.net/forum?id=_3ELRdg2sgI\n[45] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic\nChain of Thought Prompting in Large Language Models. In The Eleventh Interna-\ntional Conference on Learning Representations. https://openreview.net/forum?id=\n5NTt8GFjUHkr\n[46] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023.\nVerify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. In Pro-\nceedings of the 2023 Conference on ACL. Association for Computational Linguistics,\nToronto, Canada, 5823â€“5840. https://doi.org/10.18653/v1/2023.acl-long.320\n[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, et al. 2023. A Survey of Large Language\nModels. arXiv:2303.18223 [cs.CL]\n[48] Denny Zhou, Nathanael SchÃ¤rli, Le Hou, et al. 2022. Least-to-Most Prompting\nEnables Complex Reasoning in Large Language Models. arXiv:2205.10625 [cs.AI]\n[49] Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. 2021.\nAdaptive information seeking for open-domain question answering.\narXiv\npreprint arXiv:2109.06747 (2021).\nA\nAPPENDIX\nA.1\nCase Study\nIn this section, we compare the performance of SearChain and New\nBing 4 in adding references to supporting documents for generated\ncontent via case study. We also use case study to further analyze\nwhy CoQ has stronger reasoning ability than Baselines.\nA.1.1\nCase Study for CoQ vs Baselines in Reasoning. Baselines\nfocus on solving local sub-questions while ignoring the global plan-\nning of the reasoning chain, which leads LLM to tend to stop reason-\ning rather than try more when a sub-question cannot be solved. In\nour method, LLM acts as a commander that plans a global reasoning\nchain that can solve the complex question, when a sub-question\ncannot be solved, even without the help of IR, LLM can try to fur-\nther decompose or rewrite the sub-question to continue reasoning.\nIt is because our method focuses on building a global chain that can\nsolve the complex question (global perspective), rather than answer-\ning or generating sub-questions step by step (local perspective).\nThis makes LLM try more when faced with intermediate difficulties\nto finally solve complex questions. Case study shown in Figure 6\nindicates that SearChain continues reasoning but baselines stop.\nA.1.2\nCase Study for SearChain vs New Bing in Tracing. We com-\npare the performance of SearChain and New Bing in marking ref-\nerences for generated content via case study (Figure 7). SearChain\ncan mark references for each knowledge involved in the reason-\ning process (i.e., each correct node of CoQ) in a more fine-grained\n4https://www.bing.com/new\nThe question is incomplete and unclear about Menucha Rochel Slonim's father.\nCoT\nSelf-Ask\nw/o Retrieval\nPlease provide additional information about Menucha Rochel Slonim's father so we can \nanswer the question.\nSearChain\nw/o Retrieval\n[Query 1]: Who was the father of Menucha Rochel Slonim?\n[Answer 1]: Rabbi Dovber Schneuri, the second Rebbe of the Chabad Lubavitch movement.\n[Query 2]: Who succeeded Rabbi Dovber Schneuri as Rebbe?\n[Answer 2]: His son, Rabbi Menachem Mendel Schneersohn.\n[Final Content]: The father of Menucha Rochel Slonim is Rabbi Dovber Schneuri [1]. His \nson Rabbi Menachem Mendel Schneersohn succeeded Rabbi Dovber Schneuri as Rebbe[2]. \nSo the final answer is Richard Stallman. \nWho followed the father of Menucha Rochel Slonim?\nUnfortunately, there is not enough information provided in the question to accurately answer it. \nCoT\nSelf-Ask\nw/o Retrieval\n Follow up: What is the name of the newly declared independent country?\nIntermediate answer: The name of the country is not clear.\nSearChain\nw/o \nRetrieval\n[Query 1]: What countries are part of the commission of Truth and Friendship with Inierie? \n[Answer 1]: East Timor and Indonesia \n[Query 2]: Which of these countries recently declared independence? \n[Answer 2]: East Timor \n[Query 3]: Who is the president of East Timor? \n[Answer 3]: Francisco Guterres \n[Final Content]: East Timor and Indonesia are part of the commission of Truth and Friendship \nwith Inierie [1]. East Timor recently declared independence [2] and the president of it is \nFrancisco Guterres . So the final answer is Francisco Guterres.  \nWho is the president of the newly declared independent country that is part of the commission of Truth and \nFriendship with the country where Inierie is located?\nWilliam Beckford's child, a daughter named Maria Hamilton, the information about the death \nof Maria Hamilton is not clear \nCoT\nSelf-Ask\nw/o Retrieval\nFollow up: What was the name of William Beckford's child?\nIntermediate answer: The name of William Beckford's child is not specified in the question.\nSearChain\nw/o Retrieval\n[Query 1]: Who is William Beckford?\n[Answer 2]: William Beckford was an English novelist, art collector, and politician.\n[Query 2]: Did William Beckford have any children?\n[Answer 2]: Yes, William Beckford had one child, a son named William Thomas Beckford.\n[Query 3]: Where did William Thomas Beckford live?\n[Answer 3]: William Thomas Beckford lived in various places in England, including Bath \nand Fonthill Abbey.\n[Query 4]: Where did William Beckford's child die?\n[Answer 4]: William Thomas Beckford's child died in Bath.\n[Final Content]: William Beckford was an English novelist, art collector, and politician [1]. \nWilliam Beckford had one child, a son named William Thomas Beckford [2]. William \nThomas Beckford lived in various places in England, including Bath and Fonthill Abbey and \ndied in Bath [3,4]. So the final answer is Bath. \nWhere did William Beckford's child die?\nThe owner of the record label of the performer of \"Groovy Little Summer Song\" is not \nspecified. More information is needed to answer this question.\nCoT\nSelf-Ask\nw/o Retrieval\nFollow up: Who is the performer of Groovy Little Summer Song?\nIntermediate answer: The performer of Groovy Little Summer Song is not specified.\nSearChain\nw/o Retrieval\n[Query 1]: What is the name of the performer of Groovy Little Summer Song?\n[Answer 1]: I don't know.\n[Query 2]: Can you find out the name of the performer of Groovy Little Summer Song?\n[Answer 2]: The performer of Groovy Little Summer Song is James Otto.\n[Query 3]: What is the name of the record label that James Otto is signed to?\n[Answer 3]: Warner Music Nashville is the record label that James Otto is signed to.\n[Query 4]: Who is the owner of Warner Music Nashville?\n[Answer 4]: Warner Music Nashville is a subsidiary of Warner Music Group, so the owner \nwould be Warner Music Group.\n[Final Content]: The performer of Groovy Little Summer Song is James Otto [2]. Warner \nMusic Nashville is the record label that James Otto is signed to [3]. The owner of Warner \nMusic Nashville is Warner Music Group [4]. So the final answer is Warner Music Group .\nWho is the owner of the record label of the performer of Groovy Little Summer Song?\nFigure 6: Case study for CoQ vs Baselines in Reasoning.\nmanner. While references given by New Bing do not cover all of\nthe knowledge, and in some cases New Bing cannot find the knowl-\nedge. SearChain provides a novel perspective that decomposes com-\nplex multi-step knowledge-intensive tasks into multiple single-step\nknowledge reasoning problems, retrieving the supporting docu-\nments of knowledge for each step of reasoning, and organizing\nthese reasoning steps with their reference marks as final generated\ncontent. This enables the supporting documents to cover every\nknowledge involved in the generated content, which enhances the\ntraceability of the generated content.", "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nTable 6: Performance of SearChain and DSP on complex knowledge-intensive tasks on Vicuna-13B. Bold denotes the best result\nin different settings. FC: Fact Checking, LFQA: Long-Form QA. Metric for LFQA: ROUGE-L. Metric for others: cover-EM.\nMuti-Hop QA\nSlot Filling\nFC\nLFQA\nHoPo\nMQ\nWQA\nSQA\nzsRE\nT-REx\nFEV.\nELI5\nInteraction with Information Retrieval\nDSP\n25.45\n9.06\n27.50\n62.01\n33.71\n49.08\n73.05\n22.58\nSearChain\n29.77\n10.59\n32.32\n63.75\n36.86\n52.75\n75.47\n24.05\n      SearChain\nJoan Crawford starred in The Duke Steps Out [1], which was released on \nMay 14, 1929 [2]. The American Film Institute (AFI) ranked the greatest \nfemale Hollywood stars in 1999 [3], and the top five were Katharine \nHepburn, Bette Davis, Audrey Hepburn, Ingrid Bergman, and Greta \nGarbo [4]. Joan Crawford was ranked tenth on the list [5]. So the answer \nis 1999.\n[1] .. The Duke Steps Out The Duke Steps Out is directed by James \nCruze and starring William Haines and Joan Crawford â€¦\n[2] â€¦ The Duke Steps Out is a 1929 silent comedy-drama filmâ€¦\n[3] â€¦ AFI's 100 Years...100 Stars is a list of the top 25 male and 25 \nfemale Hollywood stars â€¦. The list was unveiled by the American Film \nInstitute on June 15, 1999 â€¦\n[4] â€¦ â€œAFIâ€˜s 100 Years...100 Starsâ€œ â€¦ Of the 25 actresses listed, Davis \nwas ranked at number two, behind Katharine Hepburn. The United States \nPostal Service honored Davis with a commemorative postage stamp in \n2008, marking the 100th anniversary of her birth â€¦\n[5] â€¦ Joan Crawford â€¦ was also voted the tenth greatest female star of \nthe classic American cinema by the American Film Institute â€¦\nThe Duke Steps Out stars an actress who was ranked tenth on a list of greatest female Hollywood stars in what year?\n      New Bing\nIâ€™m sorry, but I couldnâ€™t find any \ninformation about the actress from \nTheDuke Steps Out who was ranked \ntenth on a list of greatest female \nHollywood stars [1]. Could you provide \nmore details or perhaps rephrase your \nquestion?\n[1]: AFI's 100 Years...100 Stars is the \nAmerican Film Institute's list ranking \nthe top 25 male and 25 female greatest \nscreen legends of American film history \nand is the second list of the AFI 100 \nYears... series.\n      SearChain\nLorenzo Monaco lived and worked between 1370 and 1425 [1]. The \nfamous Italian painters and architects during his time included Giotto, \nMasaccio, Brunelleschi, and Donatello [2]. Giotto and Masaccio \nspecifically influenced his work [2], as well as Piero di Giovanni in Siena, \nSpinello Aretino, and Agnolo Gaddi [2]. So, the answer to the question \n\"Lorenzo Monaco was influenced by which Italian painter and architect?\" \nis Giotto, Masaccio, Piero di Giovanni in Siena, Spinello Aretino, and \nAgnolo Gaddi.\n[1] â€¦Lorenzo Monaco Lorenzo Monaco (born Piero di Giovanni; 1370 â€“ \nc. 1425) was an Italian painter of the â€¦\n[2] Lorenzo was an Italian painter of the late Gothic-early Renaissance \nage. He was born Piero di Giovanni in Siena. Little is known about his \nyouth, apart from the fact that he was apprenticed in Florence. He was \ninfluenced by Giotto and his followers Spinello Aretino and Agnolo \nGaddi. In 1390 he joined the Camaldolese monastery of Santa Maria \ndegli Angeli. He was thenceforth generally known as â€œLorenzo Monacoâ€ \n(English: â€œLawrence the Monkâ€). In the 1390s he executed three panels \nof the Biblioteca Laurenziana for his convent â€¦\nLorenzo Monaco was influenced by which Italian painter and architect?\n      New Bing\nLorenzo Monaco (c. 1370 â€“ c. 1425) \nwas an Italian painter of the late Gothic \nto early Renaissance age. He was born \nPiero di Giovanni in Siena, Italy and \nwas apprenticed in Florence. He was \ninfluenced by Giotto and that artistâ€™s \nfollowers Spinello Aretino and Agnolo \nGaddi1 [1].\n[1]: Lorenzo Monaco Lorenzo Monaco \nwas an Italian painter of the late Gothic-\nearly Renaissance age. He was born \nPiero di Giovanni in Siena. Little is \nknown about his youth, apart from the \nfact that he was apprenticed in Florence. \nHe was influenced by Giotto and his \nfollowers Spinello Aretino and Agnolo \nG a d d i .  I n  1 3 9 0  h e  j o i n e d  t h e \nCamaldolese â€¦\n      SearChain\nPaul Revere is Patriot in the American Revolution [1]. Mark Lindsay is a \nmusician who is the lead singer of Paul Revere & the Raiders [2]. The \nsong Arizona was recorded by Paul Revere and Mark Lindsay, and \nwritten by Kenny Young [3]. So the final answer is Kenny Young.\n[1] .. Paul Revere was an American silversmith, engraver, early \nindustrialist, and Patriot in the American Revolution. ..\n[2] â€¦ Mark Lindsay (born March 9, 1942) is an American musician, best \nknown as the lead singer of Paul Revere & the Raiders â€¦\n[3] â€¦ \"Arizona\" is a song written by Kenny Young and recorded in 1969 \nby Mark Lindsay, a solo effort while still lead singer for Paul Revere and \nthe Raiders â€¦\nThe song Arizona was recorded by Paul Revere and Mark Lindsay but who wrote the song?\n      New Bing\nThe song â€œArizonaâ€ was written by \nKenny Young and recorded by Mark \nLindsay, a solo effort while still lead \nsinger for Paul Revere and the Raiders \nin 1969 [1]. Is there anything else you \nwould like to know?\n[1]: â€¦ \"Arizona\" is a song written by \nKenny Young and recorded in 1969 by \nMark Lindsay, a solo effort while still \nlead singer for Paul Revere and the \nRaiders â€¦\n     SearChain\nThe Notorious B.I.G., also known as Christopher Wallace, was an \nAmerican rapper [1]. He released two studio albums during his lifetime: \n\"Ready to Die\" in 1994 and \"Life After Death\" in 1997 [2]. The song \n\"Unbelievable\" was included on his debut album \"Ready to Die  \n[3],â€œwhich was released on September 13, 1994 [4]. So the answer is \nSeptember 13, 1994 .\n[1] â€¦ The Notorious B.I.G. Christopher George Latore Wallace (May 21, \n1972 â€“ March 9, 1997), known professionally as the Notorious B.I.G., \nBiggie Smalls, or Biggie, was an American rapperâ€¦\n[2] â€¦ Life After Death Life After Death is the second and final studio \nalbum by American rapper The Notorious B.I.G., released on March 25, \n1997 â€¦ The Notorious B.I.G. further delving into the mafioso rap \nsubgenre. The album is a sequel to his first album, \"Ready to Dieâ€œ â€¦\n[3] â€¦ Ready to Die â€¦ Songs on the album range from homicide \nnarratives (\"Warning\") to braggadocios battle raps (\"The What,\" \n\"Unbelievable\") â€¦\n[4] â€¦ Ready to Die | Ready to Die Ready to Die is the debut studio \nalbum by American rapper The Notorious B.I.G., released on September \n13, 1994 â€¦\nWhen was the album with the song  Unbelievable by American rapper The Notorious B.I.G released?\n      New Bing\nThe song â€œUnbelievableâ€ by American \nrapper The Notorious B.I.G. was \nreleased on his debut studio album \nReady to Die on September 13, 1994 by \nBad Boy Records and Arista Records \n[1]. Is there anything else you would \nlike to know?\n[1] â€¦ Ready to Die | Ready to Die \nReady to Die is the debut studio album \nby American rapper The Notorious \nB.I.G., released on September 13, \n1994 â€¦\nFigure 7: Case study for SearChain vs New Bing in Tracing.\nTable 8: Number of examples in prompt used for in-content\nlearning on different datasets.\nMuti-Hop QA\nSlot Filling FC\nLFQA\nHoPo MQ WQA SQA zsRE T-REx FEV. ELI5\nWithout Information Retrieval\nDirect Prompting\n0\n0\n0\n0\n0\n0\n0\n0\nAuto-CoT\n4\n4\n4\n6\n4\n4\n4\n2\nCoT\n4\n4\n4\n6\n4\n4\n4\n2\nCoT-SC\n4\n4\n4\n6\n4\n4\n4\n2\nRecite-and-answer\n4\n4\n4\n6\n4\n4\n4\n2\nSelf-Ask w/o IR\n4\n4\n4\n6\n4\n4\n4\n2\nLeast-to-Most\n4\n4\n4\n6\n4\n4\n4\n2\nPlan-and-Solve\n4\n4\n4\n6\n4\n4\n4\n2\nSearChain w/o IR\n2\n2\n2\n6\n2\n2\n4\n2\nInteraction with Information Retrieval\nDirect Retrieval\n0\n0\n0\n0\n0\n0\n0\n0\nToolFormer\n4\n4\n4\n6\n4\n4\n4\n2\nSelf-Ask\n4\n4\n4\n6\n4\n4\n4\n2\nPlan-and-Solve w/ IR\n4\n4\n4\n6\n4\n4\n4\n2\nReact â†’CoT-SC\n6\n4\n4\n6\n4\n4\n4\n2\nVerify-and-Edit\n2\n2\n2\n2\n2\n2\n4\n2\nTree-of-Thought w/ IR 4\n4\n4\n6\n4\n4\n4\n2\nDSP\n16\n8\n8\n8\n8\n8\n8\n2\nSearChain\n2\n2\n2\n2\n2\n2\n4\n2\nTable 7: Performance change with ROUGE threshold.\nğ›¼= 0.30\nğ›¼= 0.35\nğ›¼= 0.40\nğ›¼= 0.45\nğ›¼= 0.50\nPerformance\n25.50\n25.57\n25.58\n25.57\n25.55\nA.2\nPerformance on Vicuna-13B\nIn this section, we compare SearChain with the competitive baseline\nDSP on Vicuna-13B 5, a strong open source large model 6 trained by\nStanford. The experimental results in Table 6 show that SearChain\nagain outperforms DSP on Vicuna-13B.\nA.3\nExperimental Details\nA.3.1\nThreshold Selection. As for the confidence threshold (ğœƒ), we\ninitialize the initial value of the confidence threshold (1.0) based\non prior knowledge and gradually increase the value with a step\nsize of 0.1. We validate the F1-score (a comprehensive metric of the", "WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua\nConstruct a global reasoning chain for this complex [Question] : \" {} \" You should generate a query to the search engine based on \nwhat you already know at each step of the reasoning chain, starting with [Query]. \nIf you know the answer for [Query], generate it starting with [Answer].\nYou can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final \nContent].\nIf you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with\n[Unsolved Query].\nFor exmaple:\n[Question]:\"Is it common to see frost during some college commencements?\"\n[Query 1]: What seasons can you expect see frost?\nIf you don't know the answer:\n[Unsolved Query]: What seasons can you expect see frost? \nInstruction: Please Stop your generation.\nIf you know the answer:\n[Answer 1]: Winter.\n[Query 2]: What months do college commencements occur?\nIf you don't know the answer:\n[Unsolved Query]: What months do college commencements occur?\nInstruction: Please Stop your generation.\nIf you know the answer:\n[Answer 2]: December, May, and sometimes June.\n[Query 3]: Do any of December, May, and sometimes June occur during winter?\nIf you don't know the answer:\n[Unsolved Query]: Do any of December, May, and sometimes June occur during winter?\nIf you know the answer:\n[Answer 3]: December\nInstruction: Please Stop your generation.\nIf you know the answer:\n[Question]:\"Is it common to see frost during some college commencements?\" (The answer can only be \"Yes\" or \"No\")\n[Final Content]: You expect see frost in Winter [1]. College commencements occur on December, May, and sometimes June [2]. \nDecember, May, and sometimes June occur during winter [3]. So the final answer is Yes.\n[Question]:\"Would a pear sink in water?\"\n[Query 1]: What is the density of a pear?\n[Answer 1]: 0.59 g/cm^3\n[Query 2]: What is the density of water?\n[Answer 2]: 1 g/cm^3\n[Query 3]: Is 0.59 g/cm^3 greater than  g/cm^3?\n[Answer 3]: No \n[Question]:\"Would a pear sink in water?\" (Yes or No)\n[Final Content]: The density of a pear is 0.59 g/cm^3 [1]. The density of water is 1 g/cm^3 [2]. 0.59 g/cm^3 is not greater than\ng/cm^3 [3]. So the final answer is No.\nFigure 10: Prompt for generating Chain-of-Query at the first\nround on StragegyQA (in the setting with information re-\ntrieval).\nConstruct a global reasoning chain for this complex [Question] : \" {} \" You should generate a query to the search engine based on \nwhat you already know at each step of the reasoning chain, starting with [Query]. \nIf you know the answer for [Query], generate it starting with [Answer].\nYou can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final \nContent].\nIf you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with\n[Unsolved Query].\nFor example:\n[Question]: \"Where do greyhound buses that are in the birthplace of Spirit If...'s performer leave from? \"\n[Query 1]: Who is the performer of Spirit If... ?\nIf you don't know the answer:\n[Unsolved Query]: Who is the performer of Spirit If... ?\nIf you know the answer:\n[Answer 1]: The performer of Spirit Ifâ€¦ is Kevin Drew.\n[Query 2]: Where was Kevin Drew born?\nIf you don't know the answer:\n[Unsolved Query]: Where was Kevin Drew born?\nIf you know the answer:\n[Answer 2]: Toronto.\n[Query 3]: Where do greyhound buses in Toronto leave from?\nIf you don't know the answer:\n[Unsolved Query]: Where do greyhound buses in Toronto leave from?\nIf you know the answer:\n[Answer 3]: Toronto Coach Terminal.\n[Final Content]: The performer of Spirit Ifâ€¦ is Kevin Drew [1]. Kevin Drew was born in Toronto [2]. Greyhound buses in \nToronto leave from Toronto \nCoach Terminal [3]. So the final answer is Toronto Coach Terminal.\n[Question]:\"Which magazine was started first Arthurâ€™s Magazine or First for Women?\"\n[Query 1]: When was Arthurâ€™s Magazine started?\n[Answer 1]: 1844.\n[Query 2]: When was First for Women started?\n[Answer 2]: 1989\n[Final Content]: Arthurâ€™s Magazine started in 1844 [1]. First for Women started in 1989 [2]. So Arthurâ€™s Magazine was started\nfirst. So the answer is Arthurâ€™s Magazine.\nFigure 11: Prompt for generating Chain-of-Query at the first\nround on HotpotQA, Musique, WikiMultiHopQA, zsRE and\nT-REx (in the setting with information retrieval).\nConstruct a global reasoning chain for this complex [Question] : \" {} \" You should generate a query to the search engine based \non what you already know at each step of the reasoning chain, starting with [Query]. \nIf you know the answer for [Query], generate it starting with [Answer].\nYou can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final \nContent].\nIf you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with\n[Unsolved Query].\nFor exmaple:\n[Question]:\"What causes the trail behind jets at high altitude?\"\n[Query 1]: What is the trail behind jets at high altitude?\nIf you don't know the answer:\n[Unsolved Query]: What is the trail behind jets at high altitude?\nIf you know the answer:\n[Answer 1]: The trail behind jets at high altitude is commonly referred to as a contrail, which is short for \"condensation trail.\" It \nis formed when the hot exhaust gases from a jet engine mix with the colder air at high altitudes, causing the water vapor in the air \nto condense and freeze into tiny ice crystals.\n[Query 2]: Why do the hot exhaust gases mix with the colder air at high altitudes?\nIf you don't know the answer:\n[Unsolved Query]: Why do the hot exhaust gases mix with the colder air at high altitudes?\nIf you know the answer:\n[Answer 2]: The hot exhaust gases from a jet engine mix with the colder air at high altitudes due to the pressure difference \nbetween the engine exhaust and the surrounding air. At high altitudes, the air is much colder and thinner, which causes the \nexhaust gases to rapidly expand and cool, creating a turbulent wake behind the aircraft.\n[Query 3]: Why does the water vapor in the air condense and freeze into ice crystals?\nIf you don't know the answer:\n[Unsolved Query]: Why does the water vapor in the air condense and freeze into ice crystals?\nIf you know the answer:\n[Answer 3]: The water vapor in the air condenses and freezes into ice crystals because the exhaust gases from the jet engine \ncontain a significant amount of water vapor, which is heated to high temperatures in the engine. When the hot water vapor mixes \nwith the cold air at high altitudes, it quickly loses heat and energy, causing the water vapor to condense into liquid droplets and \nthen freeze into ice crystals.\n[Final Content]: The trail behind jets at high altitude is formed when the hot exhaust gases from a jet engine mix with the colder \nair at high altitudes, causing the water vapor in the air to condense and freeze into tiny ice crystals. The exhaust gases mix with \nthe colder air due to the pressure difference between the engine exhaust and the surrounding air, and the water vapor condenses \nand freezes because it is heated to high temperatures in the engine and then quickly loses heat and energy when mixed with the \ncold air at high altitudes.                       \n[Question]: In Trading Places (1983, Akroyd/Murphy) how does the scheme at the end of the movie work? Why would buying a \nlot of OJ at a high price ruin the Duke Brothers?\n[Query 1]: What is the scheme at the end of Trading Places?\n[Answer 1]: In the movie, the main character, Billy Ray Valentine (Eddie Murphy), and his partner, Louis Winthorpe III (Dan \nAykroyd), execute a plan to bankrupt the Duke Brothers by manipulating the frozen concentrated orange juice (FCOJ) futures \nmarket.\n[Query 2]: How do Billy Ray and Louis manipulate the FCOJ futures market?\n[Answer 2]: Billy Ray and Louis obtain insider information about the Department of Agriculture's upcoming crop report, which \nindicates that a harsh winter has destroyed much of the orange crop. They use this information to purchase FCOJ futures \ncontracts at a low price before the report is released, and then they sell the contracts at a high price after the report's release, \nwhen the market has responded to the news of the crop damage.\n[Query 3]: Why does buying a lot of OJ at a high price ruin the Duke Brothers?\n[Answer 3]: The Duke Brothers, who are also investing in the FCOJ market, have bet that the orange crop will be abundant and \nthat the price of FCOJ will remain low. However, Billy Ray and Louis's scheme drives up the price of FCOJ, causing the Duke \nBrothers to lose a significant amount of money and ultimately leading to their downfall.\n[Final Content]: In Trading Places, Billy Ray Valentine and Louis Winthorpe III manipulate the FCOJ futures market by \nobtaining insider information about the crop report and purchasing contracts at a low price before selling them at a higher price \nafter the report is released. The Duke Brothers, who have also invested in the market, lose money because they bet on an \nabundant orange crop and low FCOJ prices. However, Billy Ray and Louis's scheme causes the price of FCOJ to rise, which \nruins the Duke Brothers and leads to their downfall.\nFigure 12: Prompt for generating Chain-of-Query at the first\nround on ELI5 (in the setting with information retrieval).\nConstruct a global reasoning chain for this complex question [Question]:\"{}\" and answer the question, and generate a query to the \nsearch engine based on what you already know at each step of the reasoning chain, starting with [Query]. \nYou should generate the answer for each [Query], starting with [Answer].\nYou should generate the final answer for the [Question] by referring the [Query]-[Answer] pairs, starting with [Final Content].\nFor exmaple:\n[Question]:\"How many places of higher learning are in the city where the Yongle emperor greeted the person to whom the edict \nwas addressed?\"\n[Query 1]: Who was the edict addressed to?\n[Answer 1]: the Karmapa\n[Query 2]: Where did the Yongle Emperor greet the Karmapa?\n[Answer 2]: Nanjing\n[Query 3]: How many places of higher learning are in Nanjing?\n[Answer 3]: 75 \n[Final Content]: The edict was addressed to Karmapa [1]. Yongle Emperor greet the Karampa in Nanjing [2]. There are 75 places \nof higher learning are in Nanjing [3]. So the final answer is 75.\n[Question]:\"Which magazine was started first Arthurâ€™s Magazine or First for Women?\"\n[Query 1]: When was Arthurâ€™s Magazine started?\n[Answer 1]: 1844.\n[Query 2]: When was First for Women started?\n[Answer 2]: 1989\n[Final Content]: Arthurâ€™s Magazine started in 1844 [1]. First for Women started in 1989 [2]. So Arthurâ€™s Magazine was started\nfirst. So the final answer is Arthurâ€™s Magazine.\nFigure 8: Prompt for generating Chain-of-Query on Hot-\npotQA, Musique, WikiMultiHopQA, zsRE and T-REx (in the\nsetting without information retrieval).", "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nConstruct a global reasoning chain for this complex question [Question]:\"{}\" and answer the question, and generate a query to\nthe search engine based on what you already know at each step of the reasoning chain, starting with [Query]. \nYou should generate the answer for each [Query], starting with [Answer].\nYou should generate the final answer for the [Question] by referring the [Query]-[Answer] pairs, starting with [Final Content].\nIf you don't know the answer, generate a query to the search engine based on what you already know and donot know, starting \nwith [Unsolved Query] and please stop your generation.\nFor exmaple:\n[Question]:\"How many places of higher learning are in the city where the Yongle emperor greeted the person to whom the edict \nwas addressed?\"\n[Query 1]: Who was the edict addressed to?\nIf you don't know the answer:\n[Unsolved Query]: Who was the edict addressed to? \nIf you know the answer:\n[Answer 1]: the Karmapa\n[Query 2]: Where did the Yongle Emperor greet the the Karmapa?\nIf you don't know the answer:\n[Unsolved Query]: here did the Yongle Emperor greet the the Karmapa?\nIf you know the answer:\n[Answer 2]: Nanjing\n[Query 3]: How many places of higher learning are in Nanjing ?\nIf you don't know the answer:\n[Unsolved Query]: How many places of higher learning are in Nanjing?\nIf you know the answer:\n[Answer 3]: 75  \n[Final Content]: The edict was addressed to Karmapa [1]. Yongle Emperor greet the Karampa in Nanjing [2]. There are 75 places \nof higher learning are in Nanjing [3]. So the final answer is 75.\n[Question]:\"Nicholas Brody is a character on Homeland. (SUPPORTS or REFUTES)?\"\n[Query 1]: What is Homeland?\n[Answer 1]: Homeland is a television series.\n[Query 2]: Is Nicholas Brody a character in Homeland?\n[Answer 2]: Yes.\n[Final Content]: Homeland is a television series [1]. Nicholas Brody is a character in Homeland [2]. So the final answer is \nSUPPORTS.\n[Question]:\"Brad Wilk helped co-found Rage in 1962. (SUPPORTS or REFUTES)?\"\n[Query 1]: Did Brad Wilk co-found Rage?\n[Answer 1]: Yes\n[Query 2]: Did Brad Wilk co-found Rage in 1962?\n[Answer 2]: No, Rage was founded in 1991\n[Final Content]: Brad Wilk did co-found Rage [1], but not in 1962 [2]. So the final answer is REFUTES.\n[Question]:\"Aristotle spent time in Athens. (SUPPORTS or REFUTES)?\"\n[Query 1]: Who is Aristotle?\n[Answer 1]: Aristotle was a Greek philosopher.\n[Query 2]: Did Aristotle spend time in Athens?\n[Answer 2]: Yes, Aristotle studied and taught at the Academy in Athens for 20 years.\n[Final Content]: Aristotle was a Greek philosopher who studied and taught at the Academy in Athens for 20 years [2]. So the final \nanswer is SUPPORTS.\n[Question]:\"Telemundo is a English-language television network. (SUPPORTS or REFUTES)?\"\n[Query 1]: What is Telemundo?\n[Answer 1]: Telemundo is a television network.\n[Query 2]: Is Telemundo an English-language television network?\n[Answer 2]: No, Telemundo is a Spanish-language television network.\n[Final Content]: Telemundo is a television network [1], but it is not an English-language television network [2]. So the final \nanswer is REFUTES.                        \nFigure 9: Prompt for generating Chain-of-Query at the first\nround on FEVER (in the setting with information retrieval).\nRecall and Precision of judging whether the passage can answer the\nquestion) on the mixed open-domain QA datasets (NQ, TriviaQA,\nWebQ, and TREC) after each value change. We find that when the\nconfidence threshold is 1.5, the highest F1-score can be achieved so\nwe set the confidence threshold as 1.5. As for the ROUGE threshold\n(ğ›¼), we determine this value by manually observing the ROUGE\nrelationship between the generated text and the ground truth in\nthe few examples in in-context learning. Our further experiments\nin Table 7 show that when the value range of ROUGE threshold is\nbetween 0.3 and 0.5, the performance change on ELI5 is not obvious.\nA.3.2\nNumber of Examples in Prompt. We show the number of ex-\namples in prompt used for in-context learning on different datasets\n(Table 8). Our method (SearChain) achieves the best performance\nwith fewer learning examples than competitive baselines.\nA.3.3\nPrompts in Experiment. We show the prompt used in experi-\nment on different datasets in Figure 8 âˆ¼12.\n5https://lmsys.org/blog/2023-03-30-vicuna/\n6https://huggingface.co/lmsys/vicuna-13b-delta-v1.1/tree/main"]